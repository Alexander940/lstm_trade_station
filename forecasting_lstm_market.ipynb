{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORECASTING CON REDES LSTM - PREDICCIONES DE MERCADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df = pd.read_csv('../data/indicators_results_10T.csv', sep=';') \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminar columna datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df = df.drop(columns=['datetime'])\n",
    "df = df.iloc[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Función para generar las particiones preservando las características\n",
    "# de la serie de tiempo\n",
    "\n",
    "def train_val_test_split(dataframe, tr_size=0.8, vl_size=0.1, ts_size=0.1 ):\n",
    "    # Definir número de datos en cada subserie\n",
    "    N = dataframe.shape[0]\n",
    "    Ntrain = int(tr_size*N)  # Número de datos de entrenamiento\n",
    "    Nval = int(vl_size*N)    # Número de datos de validación\n",
    "    Ntst = N - Ntrain - Nval # Número de datos de prueba\n",
    "\n",
    "    # Realizar partición\n",
    "    train = dataframe[0:Ntrain]\n",
    "    val = dataframe[Ntrain:Ntrain+Nval]\n",
    "    test = dataframe[Ntrain+Nval:]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "# Prueba de la función\n",
    "tr, vl, ts = train_val_test_split(df)\n",
    "\n",
    "print(f'Tamaño set de entrenamiento: {tr.shape}')\n",
    "print(f'Tamaño set de validación: {vl.shape}')\n",
    "print(f'Tamaño set de prueba: {ts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "covar = 5 # Índice de la covariable (columna del dataset) a graficar\n",
    "col = df.columns[covar]\n",
    "\n",
    "# Dibujar los sets de entrenamiento/validación/prueba para la covariable\n",
    "fig, ax = plt.subplots(figsize = (16,5))\n",
    "ax.plot(tr[col], label='Train')\n",
    "ax.plot(vl[col], label='Val')\n",
    "ax.plot(ts[col], label='Test')\n",
    "ax.set_title(f'Covariable: {col}')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear los datasets de entrenamiento, prueba y validación y verificar sus tamaños\n",
    "INPUT_LENGTH = 36    # Hiperparámetro\n",
    "OUTPUT_LENGTH = 6    # Modelo multi-step\n",
    "\n",
    "def crear_dataset_supervisado(array, input_length, output_length):\n",
    "    '''Permite crear un dataset con las entradas (X) y salidas (Y)\n",
    "    requeridas por la Red LSTM.\n",
    "\n",
    "    Parámetros:\n",
    "    - array: arreglo numpy de tamaño N x features (N: cantidad de datos,\n",
    "      f: cantidad de features)\n",
    "    - input_length: instantes de tiempo consecutivos de la(s) serie(s) de tiempo\n",
    "      usados para alimentar el modelo\n",
    "    - output_length: instantes de tiempo a pronosticar (salida del modelo)\n",
    "    '''\n",
    "\n",
    "    # Inicialización\n",
    "    X, Y = [], []    # Listados que contendrán los datos de entrada y salida del modelo\n",
    "    shape = array.shape\n",
    "    if len(shape)==1: # Si tenemos sólo una serie (univariado)\n",
    "        fils, cols = array.shape[0], 1\n",
    "        array = array.reshape(fils,cols)\n",
    "    else: # Multivariado <-- <--- ¡esta parte de la función se ejecuta en este caso!\n",
    "        fils, cols = array.shape\n",
    "\n",
    "    # Generar los arreglos\n",
    "    for i in range(fils-input_length-output_length):\n",
    "        X.append(array[i:i+INPUT_LENGTH,0:cols])\n",
    "        Y.append(array[i+input_length:i+input_length+output_length,-1].reshape(output_length,1))\n",
    "    \n",
    "    # Convertir listas a arreglos de NumPy\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_tr, y_tr = crear_dataset_supervisado(tr.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "x_vl, y_vl = crear_dataset_supervisado(vl.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "x_ts, y_ts = crear_dataset_supervisado(ts.values, INPUT_LENGTH, OUTPUT_LENGTH)\n",
    "\n",
    "# Imprimir información en pantalla\n",
    "print('Tamaños entrada (BATCHES x INPUT_LENGTH x FEATURES) y de salida (BATCHES x OUTPUT_LENGTH x FEATURES)')\n",
    "print(f'Set de entrenamiento - x_tr: {x_tr.shape}, y_tr: {y_tr.shape}')\n",
    "print(f'Set de validación - x_vl: {x_vl.shape}, y_vl: {y_vl.shape}')\n",
    "print(f'Set de prueba - x_ts: {x_ts.shape}, y_ts: {y_ts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def escalar_dataset(data_input, col_ref):\n",
    "    '''Escala el dataset en el rango de -1 a 1.\n",
    "\n",
    "    Entradas:\n",
    "    - data_input: diccionario con los dataset de entrada y salida del modelo\n",
    "    (data_input = {'x_tr':x_tr, 'y_tr':y_tr, 'x_vl':x_vl, 'y_vl':y_vl,\n",
    "                    'y_ts':y_ts})\n",
    "    - col_ref: parámetro adicional para especificar la columna que contiene\n",
    "      la variable a predecir\n",
    "\n",
    "    \n",
    "    Retorna:\n",
    "    - data_scaled: diccionario con los datasets de entrada y salida escalados\n",
    "      (tiene la misma estructura del diccionario de entrada)\n",
    "    - scaler: el escalador usado (requerido para las predicciones)\n",
    "    '''\n",
    "\n",
    "    # *** Implementación adicional: determinar el índice de la columna\n",
    "    # que contiene la variable a predecir\n",
    "    col_ref = df.columns.get_loc(col_ref)\n",
    "\n",
    "    # Número de instantes de tiempo de entrada y de covariables\n",
    "    NSAMPLES = data_input['x_tr'].shape[1]\n",
    "    NFEATS = data_input['x_tr'].shape[2]\n",
    "\n",
    "    # Generar listado con \"scalers\" (1 por cada covariable de entrada)\n",
    "    scalers = [MinMaxScaler(feature_range=(-1,1)) for i in range(NFEATS)]\n",
    "\n",
    "    # Arreglos que contendrán los datasets escalados\n",
    "    x_tr_s = np.zeros(data_input['x_tr'].shape)\n",
    "    x_vl_s = np.zeros(data_input['x_vl'].shape)\n",
    "    x_ts_s = np.zeros(data_input['x_ts'].shape)\n",
    "    y_tr_s = np.zeros(data_input['y_tr'].shape)\n",
    "    y_vl_s = np.zeros(data_input['y_vl'].shape)\n",
    "    y_ts_s = np.zeros(data_input['y_ts'].shape)\n",
    "\n",
    "    # Escalamiento: se usarán los min/max del set de entrenamiento para\n",
    "    # escalar la totalidad de los datasets\n",
    "\n",
    "    # Escalamiento Xs\n",
    "    for i in range(NFEATS):\n",
    "        x_tr_s[:,:,i] = scalers[i].fit_transform(x_tr[:,:,i])\n",
    "        x_vl_s[:,:,i] = scalers[i].transform(x_vl[:,:,i])\n",
    "        x_ts_s[:,:,i] = scalers[i].transform(x_ts[:,:,i])\n",
    "    \n",
    "    # Escalamiento Ys (teniendo en cuenta \"col_ind\")\n",
    "    y_tr_s[:,:,0] = scalers[col_ref].fit_transform(y_tr[:,:,0])\n",
    "    y_vl_s[:,:,0] = scalers[col_ref].transform(y_vl[:,:,0])\n",
    "    y_ts_s[:,:,0] = scalers[col_ref].transform(y_ts[:,:,0])\n",
    "\n",
    "    # Conformar diccionario de salida\n",
    "    data_scaled = {\n",
    "        'x_tr_s': x_tr_s, 'y_tr_s': y_tr_s,\n",
    "        'x_vl_s': x_vl_s, 'y_vl_s': y_vl_s,\n",
    "        'x_ts_s': x_ts_s, 'y_ts_s': y_ts_s,\n",
    "    }\n",
    "\n",
    "    return data_scaled, scalers[col_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamiento del dataset con la función anterior\n",
    "\n",
    "# Crear diccionario de entrada\n",
    "data_in = {\n",
    "    'x_tr': x_tr, 'y_tr': y_tr,\n",
    "    'x_vl': x_vl, 'y_vl': y_vl,\n",
    "    'x_ts': x_ts, 'y_ts': y_ts,\n",
    "}\n",
    "\n",
    "# Y escalar (especificando la columna con la variable a predecir)\n",
    "data_s, scaler = escalar_dataset(data_in, col_ref = 'close')\n",
    "\n",
    "# Extraer subsets escalados\n",
    "x_tr_s, y_tr_s = data_s['x_tr_s'], data_s['y_tr_s']\n",
    "x_vl_s, y_vl_s = data_s['x_vl_s'], data_s['y_vl_s']\n",
    "x_ts_s, y_ts_s = data_s['x_ts_s'], data_s['y_ts_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Y generemos una gráfica tipo violín para ver la distribución\n",
    "# de los valores en cada covariable (entrada) y en la variable a\n",
    "# predecir (salida)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "for i in range(13):\n",
    "    ax.violinplot(dataset=x_tr_s[:,:,i].flatten(), positions=[i])\n",
    "    ax.violinplot(dataset=x_vl_s[:,:,i].flatten(), positions=[i])\n",
    "    ax.violinplot(dataset=x_ts_s[:,:,i].flatten(), positions=[i])\n",
    "\n",
    "# Etiquetas eje horizontal\n",
    "ax.set_xticks(list(range(13)))\n",
    "ax.set_xticklabels(df.keys(), rotation=90)\n",
    "ax.autoscale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Y hagamos lo mismo con la variable de salida:\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.violinplot(dataset=y_tr_s.flatten())\n",
    "ax.violinplot(dataset=y_vl_s.flatten())\n",
    "ax.violinplot(dataset=y_ts_s.flatten())\n",
    "ax.set_xticks([1])\n",
    "ax.set_xticklabels(['y (salida)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Creación del modelo\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ajustar parámetros para reproducibilidad del entrenamiento\n",
    "tf.random.set_seed(123)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# El modelo\n",
    "N_UNITS = 128 # Tamaño del estado oculto (h) y de la celdad de memoria (c) (128)\n",
    "INPUT_SHAPE = (x_tr_s.shape[1], x_tr_s.shape[2]) # 24 (horas) x 13 (features)\n",
    "\n",
    "modelo = Sequential()\n",
    "modelo.add(LSTM(N_UNITS, input_shape=INPUT_SHAPE))\n",
    "\n",
    "# Y lo único que cambia con respecto al modelo multivariado + multi-step es\n",
    "# el tamaño deldato de salida (4 horas)\n",
    "modelo.add(Dense(OUTPUT_LENGTH, activation='linear')) # activation = 'linear' pues queremos pronosticar (regresión)\n",
    "\n",
    "# Pérdida: se usará el RMSE (root mean squared error) para el entrenamiento\n",
    "# pues permite tener errores en las mismas unidades de la temperatura\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse\n",
    "\n",
    "# Compilación\n",
    "optimizador = RMSprop(learning_rate=5e-4) # 5e-5\n",
    "modelo.compile(\n",
    "    optimizer = optimizador,\n",
    "    loss = root_mean_squared_error,\n",
    ")\n",
    "\n",
    "# Entrenamiento (aproximadamente 1 min usando GPU)\n",
    "EPOCHS = 80 # Hiperparámetro\n",
    "BATCH_SIZE = 256 # Hiperparámetro\n",
    "historia = modelo.fit(\n",
    "    x = x_tr_s,\n",
    "    y = y_tr_s,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (x_vl_s, y_vl_s),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    " # Graficar curvas de entrenamiento y validación\n",
    " # para verificar que no existe overfitting\n",
    "plt.plot(historia.history['loss'],label='RMSE train')\n",
    "plt.plot(historia.history['val_loss'],label='RMSE val')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cálculo de rmses para train, val y test\n",
    "rmse_tr = modelo.evaluate(x=x_tr_s, y=y_tr_s, verbose=0)\n",
    "rmse_vl = modelo.evaluate(x=x_vl_s, y=y_vl_s, verbose=0)\n",
    "rmse_ts = modelo.evaluate(x=x_ts_s, y=y_ts_s, verbose=0)\n",
    "\n",
    "# Imprimir resultados en pantalla\n",
    "print('Comparativo desempeños:')\n",
    "print(f'  RMSE train:\\t {rmse_tr:.3f}')\n",
    "print(f'  RMSE val:\\t {rmse_vl:.3f}')\n",
    "print(f'  RMSE test:\\t {rmse_ts:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 1. Generar las predicciones sobre el set de prueba\n",
    "y_ts_pred_s = modelo.predict(x_ts_s, verbose=0)\n",
    "\n",
    "# 2. Realizar la transformación inversa de las predicciones para llevar sus\n",
    "# valores a la escala original\n",
    "y_ts_pred = scaler.inverse_transform(y_ts_pred_s)\n",
    "\n",
    "# 3. Calcular RMSE para cada instante de tiempo predicho\n",
    "diff_cuad = np.square(y_ts.squeeze()-y_ts_pred) # BATCHESx4\n",
    "proms = np.mean(diff_cuad, axis=0) # 1x4\n",
    "rmse = np.sqrt(proms) # 1x4\n",
    "\n",
    "# Graficar rmse para cada timestep\n",
    "t = np.linspace(1,4,4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(t,rmse)\n",
    "ax.set_xlabel('Hora predicha')\n",
    "ax.set_ylabel('Error RMSE (°C)')\n",
    "plt.xticks(ticks=t, labels=t)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(x, model, scaler):\n",
    "    '''Genera la predicción de OUTPUT_LENGTH instantes\n",
    "    de tiempo a futuro con el modelo entrenado.\n",
    "\n",
    "    Entrada:\n",
    "    - x: batch (o batches) de datos para ingresar al modelo\n",
    "      (tamaño: BATCHES X INPUT_LENGTH X FEATURES)\n",
    "    - model: Red LSTM entrenada\n",
    "    - scaler: escalador (requerido para llevar la predicción a la escala original)\n",
    "    \n",
    "    Salida:\n",
    "    - y_pred: la predicción en la escala original (tamaño: BATCHES X OUTPUT_LENGTH X FEATURES)\n",
    "    '''\n",
    "\n",
    "    # Calcular predicción escalada en el rango de -1 a 1\n",
    "    y_pred_s = model.predict(x,verbose=0)\n",
    "\n",
    "    # Llevar la predicción a la escala original\n",
    "    y_pred = scaler.inverse_transform(y_pred_s)\n",
    "\n",
    "    return y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular predicciones sobre el set de prueba\n",
    "y_ts_pred = predecir(x_ts_s, modelo, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "N = len(y_ts_pred)    # Número de predicciones\n",
    "ndato = np.linspace(1,N,N)\n",
    "\n",
    "# Cálculo de errores simples\n",
    "errores = y_ts.flatten()-y_ts_pred.flatten()\n",
    "plt.plot(errores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
